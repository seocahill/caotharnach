<!-- index.erb -->
<!DOCTYPE html>
<html color-mode="user">
  <head>
    <title>Speech Recognition and Text-to-Speech App</title>
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <link rel="stylesheet" href="https://unpkg.com/mvp.css">
  </head>
  <body>
    <main>
      <h1>Cuir ceist ar an gCaotharnach!</h1>
      <p>Sampla: "Cén t-ainm atá ort?"</p>
      <input type="button" id="startBtn" value="Start Recording">
      <hr>
      <h2>Comhrá</h2>
      <ul id="conversationList"></ul>
      <input type="button" onClick="window.location.reload();" value="Glan an comhrá">
      <hr>
    </main>
    <footer>
      <p>
        <a href="https://github.com/seocahill/caotharnach">Fórc anseo.</a>
      </p>
    </footer>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script>
      $(document).ready(function() {
          const startBtn = document.getElementById('startBtn');
          const conversationList = document.getElementById('conversationList');
          let mediaRecorder;
          let isRecording = false;

          startBtn.addEventListener('click', toggleRecording);

          async function toggleRecording(e) {
            try {
                if (!isRecording) {
                  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                  mediaRecorder = new MediaRecorder(stream);
                  let audioChunks = [];

                  mediaRecorder.ondataavailable = event => {
                      if (event.data.size > 0) {
                          audioChunks.push(event.data);
                      }
                  };

                  mediaRecorder.onstop = async () => {
                      const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                      await processAudio(audioBlob);
                  };

                  startBtn.value = 'Stop Recording';
                  isRecording = true;
                  mediaRecorder.start();

                  setTimeout(() => {
                      mediaRecorder.stop();
                      startBtn.value = 'Start Recording';
                      isRecording = false;
                  }, 20000); // Stop recording after 20 seconds
                } else {
                    mediaRecorder.stop();
                    startBtn.value = 'Start Recording';
                    isRecording = false;
                }
            } catch (error) {
              alert('Abó go Deo! Tá an Chaotharnach briste!');
            }
          }

          async function processAudio(audioBlob) {
            try {
                const audioString = await blobToBase64(audioBlob);
                const response = await fetch('/forward_audio', {
                  method: 'POST',
                  body: JSON.stringify({ audio_blob: audioString }),
                  headers: {
                  'Content-Type': 'application/json'
                  }
                });

                const data = await response.json();
                // Assuming gaelicResponse is the object you received with audioContent
                const audioContent = data.audioContent;

                // Convert Base64 audio content to Blob
                const decodedAudioBlob = base64ToBlob(audioContent);

                // Create an audio element
                const audio = new Audio();

                // Create a Blob URL from the audio Blob
                const blobUrl = URL.createObjectURL(decodedAudioBlob);

                // Set the audio source to the Blob URL
                audio.src = blobUrl;

                // Play the audio
                audio.play();

                // Update converstaion
                updateConversation()

            } catch (error) {
                console.error('Error processing audio:', error);
            }
        }

        async function updateConversation() {
          try {
              const response = await fetch('/get_context');
              const contextData = await response.json();

              // Assuming contextData is an array containing the last two context items
              contextData.forEach(item => {
                  const listItem = document.createElement('li');
                  listItem.textContent = JSON.stringify(item);;
                  conversationList.appendChild(listItem);
              });
          } catch (error) {
              console.error('Error fetching context:', error);
          }
      }

        function base64ToBlob(base64) {
            const binaryString = atob(base64);
            const bytes = new Uint8Array(binaryString.length);
            for (let i = 0; i < binaryString.length; i++) {
                bytes[i] = binaryString.charCodeAt(i);
            }
            return new Blob([bytes], { type: 'audio/wav' });
        }

        async function blobToBase64(blob) {
          return new Promise((resolve, reject) => {
              const reader = new FileReader();
              reader.onload = () => resolve(reader.result.split(',')[1]);
              reader.onerror = error => reject(error);
              reader.readAsDataURL(blob);
          });
        }


        async function chatWithGpt(userInput) {
            const response = await fetch('/chat_with_gpt', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({ user_input: userInput })
            });

            return await response.json();
        }
    });
    </script>
  </body>
</html>
